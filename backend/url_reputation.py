"""
PayGuard V2 URL Reputation Service

This module implements a URL reputation service that:
- Integrates with multiple threat intelligence feeds (OpenPhish, PhishTank, URLhaus)
- Uses bloom filters for fast local lookups
- Checks domain age to flag newly registered domains
- Inspects SSL certificates for validity and organization match
- Maintains a whitelist for verified domains to reduce false positives

Requirements: 12.1, 12.2, 12.3, 12.4, 12.7, 12.9, 12.10
"""

import asyncio
import hashlib
import logging
import socket
import ssl
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from enum import Enum
from typing import Any, Dict, List, Optional, Set, Tuple
from urllib.parse import urlparse

import httpx

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class ThreatType(Enum):
    """Types of threats detected"""

    PHISHING = "phishing"
    MALWARE = "malware"
    SCAM = "scam"
    SPAM = "spam"
    UNKNOWN = "unknown"


class ThreatSource(Enum):
    """Threat intelligence feed sources"""

    OPENPHISH = "openphish"
    PHISHTANK = "phishtank"
    URLHAUS = "urlhaus"
    LOCAL_BLOCKLIST = "local_blocklist"
    DOMAIN_AGE = "domain_age"
    SSL_INSPECTION = "ssl_inspection"


@dataclass
class SSLInfo:
    """SSL certificate information"""

    valid: bool
    issuer: str
    expires_at: Optional[datetime]
    organization_match: bool
    common_name: Optional[str] = None
    subject_alt_names: List[str] = field(default_factory=list)
    error: Optional[str] = None


@dataclass
class ReputationResult:
    """Result of a URL reputation check"""

    url: str
    domain: str
    is_malicious: bool
    threat_type: Optional[ThreatType] = None
    sources: List[str] = field(default_factory=list)
    domain_age_days: Optional[int] = None
    ssl_info: Optional[SSLInfo] = None
    cached: bool = False
    checked_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    confidence: float = 0.0
    is_whitelisted: bool = False


@dataclass
class CacheStats:
    """Statistics about the threat cache"""

    total_entries: int
    bloom_filter_size: int
    last_update: Optional[datetime]
    openphish_count: int
    phishtank_count: int
    urlhaus_count: int
    false_positive_rate: float


@dataclass
class UpdateResult:
    """Result of a cache update operation"""

    success: bool
    entries_added: int
    entries_removed: int
    duration_seconds: float
    errors: List[str] = field(default_factory=list)


# ============= Bloom Filter Implementation =============


class BloomFilter:
    """
    Simple bloom filter for fast URL lookups.

    Requirement 12.4: Use bloom filters for fast local lookups
    """

    def __init__(self, size: int = 1_000_000, hash_count: int = 7):
        """
        Initialize bloom filter.

        Args:
            size: Number of bits in the filter
            hash_count: Number of hash functions to use
        """
        self.size = size
        self.hash_count = hash_count
        self.bit_array = bytearray((size + 7) // 8)
        self.item_count = 0

    def _get_hash_values(self, item: str) -> List[int]:
        """Generate hash values for an item"""
        hashes = []
        for i in range(self.hash_count):
            # Use different seeds for each hash
            h = hashlib.sha256(f"{i}:{item}".encode()).hexdigest()
            hashes.append(int(h, 16) % self.size)
        return hashes

    def add(self, item: str) -> None:
        """Add an item to the bloom filter"""
        for pos in self._get_hash_values(item):
            byte_pos = pos // 8
            bit_pos = pos % 8
            self.bit_array[byte_pos] |= 1 << bit_pos
        self.item_count += 1

    def contains(self, item: str) -> bool:
        """Check if an item might be in the bloom filter"""
        for pos in self._get_hash_values(item):
            byte_pos = pos // 8
            bit_pos = pos % 8
            if not (self.bit_array[byte_pos] & (1 << bit_pos)):
                return False
        return True

    def clear(self) -> None:
        """Clear the bloom filter"""
        self.bit_array = bytearray((self.size + 7) // 8)
        self.item_count = 0

    @property
    def estimated_false_positive_rate(self) -> float:
        """Estimate the current false positive rate"""
        if self.item_count == 0:
            return 0.0
        # Formula: (1 - e^(-kn/m))^k
        import math

        k = self.hash_count
        n = self.item_count
        m = self.size
        return (1 - math.exp(-k * n / m)) ** k


# ============= Threat Feed Interfaces =============


class ThreatFeed(ABC):
    """Abstract base class for threat intelligence feeds"""

    @property
    @abstractmethod
    def name(self) -> str:
        """Name of the feed"""
        pass

    @property
    @abstractmethod
    def source(self) -> ThreatSource:
        """Source identifier"""
        pass

    @abstractmethod
    async def fetch_threats(self) -> List[str]:
        """Fetch list of malicious URLs from the feed"""
        pass

    @abstractmethod
    async def check_url(self, url: str) -> Optional[ThreatType]:
        """Check if a URL is in this feed's threat list"""
        pass


class OpenPhishFeed(ThreatFeed):
    """
    OpenPhish threat intelligence feed.

    Requirement 12.1: Integrate with minimum 3 vetted threat feeds
    """

    FEED_URL = "https://openphish.com/feed.txt"

    def __init__(self):
        self._urls: Set[str] = set()
        self._last_fetch: Optional[datetime] = None
        self._bloom = BloomFilter(size=500_000)

    @property
    def name(self) -> str:
        return "OpenPhish"

    @property
    def source(self) -> ThreatSource:
        return ThreatSource.OPENPHISH

    async def fetch_threats(self) -> List[str]:
        """Fetch phishing URLs from OpenPhish"""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(
                    self.FEED_URL, headers={"User-Agent": "PayGuard/2.0 ThreatIntel"}
                )
                response.raise_for_status()

                urls = []
                for line in response.text.strip().split("\n"):
                    url = line.strip()
                    if url and url.startswith("http"):
                        urls.append(url)
                        self._urls.add(url)
                        self._bloom.add(url)
                        # Also add domain
                        domain = self._extract_domain(url)
                        if domain:
                            self._bloom.add(domain)

                self._last_fetch = datetime.now(timezone.utc)
                logger.info(f"OpenPhish: Fetched {len(urls)} URLs")
                return urls

        except Exception as e:
            logger.error(f"OpenPhish fetch failed: {e}")
            return []

    async def check_url(self, url: str) -> Optional[ThreatType]:
        """Check if URL is in OpenPhish feed"""
        # Fast bloom filter check first
        if self._bloom.contains(url):
            if url in self._urls:
                return ThreatType.PHISHING

        # Also check domain
        domain = self._extract_domain(url)
        if domain and self._bloom.contains(domain):
            # Check if any URL with this domain is in the set
            for known_url in self._urls:
                if domain in known_url:
                    return ThreatType.PHISHING

        return None

    def _extract_domain(self, url: str) -> Optional[str]:
        """Extract domain from URL"""
        try:
            parsed = urlparse(url)
            return parsed.netloc.lower()
        except Exception:
            return None


class PhishTankFeed(ThreatFeed):
    """
    PhishTank threat intelligence feed.

    Requirement 12.1: Integrate with minimum 3 vetted threat feeds
    """

    # PhishTank requires API key for full access, using public verified list
    FEED_URL = "http://data.phishtank.com/data/online-valid.json"

    def __init__(self, api_key: Optional[str] = None):
        self._urls: Set[str] = set()
        self._domains: Set[str] = set()
        self._last_fetch: Optional[datetime] = None
        self._bloom = BloomFilter(size=500_000)
        self._api_key = api_key

    @property
    def name(self) -> str:
        return "PhishTank"

    @property
    def source(self) -> ThreatSource:
        return ThreatSource.PHISHTANK

    async def fetch_threats(self) -> List[str]:
        """Fetch phishing URLs from PhishTank"""
        try:
            headers = {"User-Agent": "PayGuard/2.0 ThreatIntel"}

            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.get(self.FEED_URL, headers=headers)
                response.raise_for_status()

                data = response.json()
                urls = []

                for entry in data:
                    url = entry.get("url", "")
                    if url and url.startswith("http"):
                        urls.append(url)
                        self._urls.add(url)
                        self._bloom.add(url)

                        # Extract and store domain
                        domain = self._extract_domain(url)
                        if domain:
                            self._domains.add(domain)
                            self._bloom.add(domain)

                self._last_fetch = datetime.now(timezone.utc)
                logger.info(f"PhishTank: Fetched {len(urls)} URLs")
                return urls

        except Exception as e:
            logger.error(f"PhishTank fetch failed: {e}")
            return []

    async def check_url(self, url: str) -> Optional[ThreatType]:
        """Check if URL is in PhishTank feed"""
        if self._bloom.contains(url):
            if url in self._urls:
                return ThreatType.PHISHING

        domain = self._extract_domain(url)
        if domain and domain in self._domains:
            return ThreatType.PHISHING

        return None

    def _extract_domain(self, url: str) -> Optional[str]:
        """Extract domain from URL"""
        try:
            parsed = urlparse(url)
            return parsed.netloc.lower()
        except Exception:
            return None


class URLhausFeed(ThreatFeed):
    """
    URLhaus threat intelligence feed (malware distribution URLs).

    Requirement 12.1: Integrate with minimum 3 vetted threat feeds
    """

    FEED_URL = "https://urlhaus.abuse.ch/downloads/text_online/"

    def __init__(self):
        self._urls: Set[str] = set()
        self._last_fetch: Optional[datetime] = None
        self._bloom = BloomFilter(size=500_000)

    @property
    def name(self) -> str:
        return "URLhaus"

    @property
    def source(self) -> ThreatSource:
        return ThreatSource.URLHAUS

    async def fetch_threats(self) -> List[str]:
        """Fetch malware URLs from URLhaus"""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(
                    self.FEED_URL, headers={"User-Agent": "PayGuard/2.0 ThreatIntel"}
                )
                response.raise_for_status()

                urls = []
                for line in response.text.strip().split("\n"):
                    line = line.strip()
                    # Skip comments
                    if line.startswith("#") or not line:
                        continue
                    if line.startswith("http"):
                        urls.append(line)
                        self._urls.add(line)
                        self._bloom.add(line)

                        domain = self._extract_domain(line)
                        if domain:
                            self._bloom.add(domain)

                self._last_fetch = datetime.now(timezone.utc)
                logger.info(f"URLhaus: Fetched {len(urls)} URLs")
                return urls

        except Exception as e:
            logger.error(f"URLhaus fetch failed: {e}")
            return []

    async def check_url(self, url: str) -> Optional[ThreatType]:
        """Check if URL is in URLhaus feed"""
        if self._bloom.contains(url):
            if url in self._urls:
                return ThreatType.MALWARE

        return None

    def _extract_domain(self, url: str) -> Optional[str]:
        """Extract domain from URL"""
        try:
            parsed = urlparse(url)
            return parsed.netloc.lower()
        except Exception:
            return None


# ============= Domain Age Checker =============


class DomainAgeChecker:
    """
    Check domain registration age.

    Requirement 12.7: Flag domains < 30 days old
    """

    # Threshold for flagging new domains (in days)
    NEW_DOMAIN_THRESHOLD = 30

    def __init__(self):
        self._cache: Dict[str, Tuple[Optional[int], datetime]] = {}
        self._cache_ttl = timedelta(hours=24)

    async def get_domain_age(self, domain: str) -> Optional[int]:
        """
        Get domain age in days.

        Args:
            domain: Domain to check

        Returns:
            Age in days, or None if unable to determine
        """
        # Check cache first
        if domain in self._cache:
            age, cached_at = self._cache[domain]
            if datetime.now(timezone.utc) - cached_at < self._cache_ttl:
                return age

        try:
            import whois

            w = whois.whois(domain)

            creation_date = w.creation_date
            if isinstance(creation_date, list):
                creation_date = creation_date[0]

            if creation_date:
                age_days = (datetime.now(timezone.utc) - creation_date).days
                self._cache[domain] = (age_days, datetime.now(timezone.utc))
                return age_days

        except Exception as e:
            logger.debug(f"WHOIS lookup failed for {domain}: {e}")

        self._cache[domain] = (None, datetime.now(timezone.utc))
        return None

    def is_new_domain(self, age_days: Optional[int]) -> bool:
        """Check if domain is considered new (< 30 days)"""
        if age_days is None:
            return False  # Can't determine, don't flag
        return age_days < self.NEW_DOMAIN_THRESHOLD


# ============= SSL Certificate Inspector =============


class SSLInspector:
    """
    Inspect SSL certificates for validity and organization match.

    Requirement 12.9: Check issuer, validity, organization match
    """

    def __init__(self):
        self._cache: Dict[str, Tuple[SSLInfo, datetime]] = {}
        self._cache_ttl = timedelta(hours=1)

    async def inspect_certificate(self, domain: str) -> SSLInfo:
        """
        Inspect SSL certificate for a domain.

        Args:
            domain: Domain to inspect

        Returns:
            SSLInfo with certificate details
        """
        # Check cache
        if domain in self._cache:
            info, cached_at = self._cache[domain]
            if datetime.now(timezone.utc) - cached_at < self._cache_ttl:
                return info

        try:
            # Create SSL context
            context = ssl.create_default_context()

            # Connect and get certificate
            with socket.create_connection((domain, 443), timeout=10) as sock:
                with context.wrap_socket(sock, server_hostname=domain) as ssock:
                    cert = ssock.getpeercert()

            # Parse certificate info
            issuer_dict = dict(x[0] for x in cert.get("issuer", []))
            issuer = issuer_dict.get("organizationName", "Unknown")

            subject_dict = dict(x[0] for x in cert.get("subject", []))
            common_name = subject_dict.get("commonName", "")
            organization = subject_dict.get("organizationName", "")

            # Get expiration date
            not_after = cert.get("notAfter")
            expires_at = None
            if not_after:
                expires_at = datetime.strptime(not_after, "%b %d %H:%M:%S %Y %Z")

            # Get Subject Alternative Names
            san_list = []
            for san_type, san_value in cert.get("subjectAltName", []):
                if san_type == "DNS":
                    san_list.append(san_value)

            # Check organization match
            org_match = self._check_organization_match(
                domain, organization, common_name
            )

            # Check if certificate is valid
            is_valid = True
            if expires_at and expires_at < datetime.now(timezone.utc):
                is_valid = False

            # Check if domain matches CN or SAN
            domain_matches = self._domain_matches_cert(domain, common_name, san_list)
            if not domain_matches:
                is_valid = False

            ssl_info = SSLInfo(
                valid=is_valid,
                issuer=issuer,
                expires_at=expires_at,
                organization_match=org_match,
                common_name=common_name,
                subject_alt_names=san_list,
            )

            self._cache[domain] = (ssl_info, datetime.now(timezone.utc))
            return ssl_info

        except ssl.SSLCertVerificationError as e:
            ssl_info = SSLInfo(
                valid=False,
                issuer="Unknown",
                expires_at=None,
                organization_match=False,
                error=f"Certificate verification failed: {e}",
            )
            self._cache[domain] = (ssl_info, datetime.now(timezone.utc))
            return ssl_info

        except Exception as e:
            ssl_info = SSLInfo(
                valid=False,
                issuer="Unknown",
                expires_at=None,
                organization_match=False,
                error=f"SSL inspection failed: {e}",
            )
            self._cache[domain] = (ssl_info, datetime.now(timezone.utc))
            return ssl_info

    def _domain_matches_cert(
        self, domain: str, common_name: str, san_list: List[str]
    ) -> bool:
        """Check if domain matches certificate CN or SAN"""
        domain = domain.lower()

        # Check common name
        if self._matches_pattern(domain, common_name.lower()):
            return True

        # Check SANs
        for san in san_list:
            if self._matches_pattern(domain, san.lower()):
                return True

        return False

    def _matches_pattern(self, domain: str, pattern: str) -> bool:
        """Check if domain matches a certificate pattern (supports wildcards)"""
        if pattern.startswith("*."):
            # Wildcard certificate
            suffix = pattern[2:]
            return domain.endswith(suffix) or domain == suffix
        return domain == pattern

    def _check_organization_match(
        self, domain: str, organization: str, common_name: str
    ) -> bool:
        """
        Check if certificate organization matches the domain.

        This is a heuristic check - legitimate sites usually have
        organization names that relate to their domain.
        """
        if not organization:
            return False

        domain_parts = domain.lower().replace(".", " ").split()
        org_lower = organization.lower()

        # Check if any significant part of domain appears in organization
        for part in domain_parts:
            if len(part) > 3 and part in org_lower:
                return True

        return False


# ============= Whitelist Manager =============


class WhitelistManager:
    """
    Manage whitelist of verified domains.

    Requirement 12.10: Whitelist for verified domains to reduce false positives
    """

    # Default trusted domains
    DEFAULT_WHITELIST = {
        # Major tech companies
        "google.com",
        "www.google.com",
        "microsoft.com",
        "www.microsoft.com",
        "apple.com",
        "www.apple.com",
        "amazon.com",
        "www.amazon.com",
        "facebook.com",
        "www.facebook.com",
        "twitter.com",
        "www.twitter.com",
        "linkedin.com",
        "www.linkedin.com",
        "github.com",
        "www.github.com",
        # Major banks (examples)
        "chase.com",
        "www.chase.com",
        "bankofamerica.com",
        "www.bankofamerica.com",
        "wellsfargo.com",
        "www.wellsfargo.com",
        # Payment processors
        "paypal.com",
        "www.paypal.com",
        "stripe.com",
        "www.stripe.com",
        "square.com",
        "www.square.com",
        # Email providers
        "gmail.com",
        "mail.google.com",
        "outlook.com",
        "outlook.live.com",
        "yahoo.com",
        "mail.yahoo.com",
    }

    def __init__(self, custom_whitelist: Optional[Set[str]] = None):
        self._whitelist: Set[str] = self.DEFAULT_WHITELIST.copy()
        if custom_whitelist:
            self._whitelist.update(custom_whitelist)
        self._custom_entries: Set[str] = custom_whitelist or set()

    def is_whitelisted(self, domain: str) -> bool:
        """Check if a domain is whitelisted"""
        domain = domain.lower()

        # Direct match
        if domain in self._whitelist:
            return True

        # Check if it's a subdomain of a whitelisted domain
        for whitelisted in self._whitelist:
            if domain.endswith("." + whitelisted):
                return True

        return False

    def add_to_whitelist(self, domain: str) -> None:
        """Add a domain to the whitelist"""
        domain = domain.lower()
        self._whitelist.add(domain)
        self._custom_entries.add(domain)
        logger.info(f"Added {domain} to whitelist")

    def remove_from_whitelist(self, domain: str) -> bool:
        """Remove a domain from the whitelist"""
        domain = domain.lower()
        if domain in self._custom_entries:
            self._whitelist.discard(domain)
            self._custom_entries.discard(domain)
            logger.info(f"Removed {domain} from whitelist")
            return True
        return False

    def get_whitelist(self) -> Set[str]:
        """Get all whitelisted domains"""
        return self._whitelist.copy()

    def get_custom_entries(self) -> Set[str]:
        """Get custom whitelist entries (not default)"""
        return self._custom_entries.copy()


# ============= Main URL Reputation Service =============


class URLReputationService:
    """
    URL Reputation Service for PayGuard V2.

    Integrates multiple threat intelligence feeds with local caching
    for fast, reliable URL reputation checking.

    Requirements:
        - 12.1: Integrate with minimum 3 vetted threat feeds
        - 12.2: Update threat data incrementally every 15 minutes
        - 12.3: Cache threat data locally for offline use
        - 12.4: Use bloom filters for fast local lookups
        - 12.7: Flag domains < 30 days old
        - 12.9: Check SSL certificate issuer, validity, organization match
        - 12.10: Whitelist for verified domains to reduce false positives
    """

    # Update interval in seconds (15 minutes)
    UPDATE_INTERVAL = 15 * 60

    def __init__(
        self,
        phishtank_api_key: Optional[str] = None,
        custom_whitelist: Optional[Set[str]] = None,
        enable_domain_age_check: bool = True,
        enable_ssl_inspection: bool = True,
    ):
        """
        Initialize the URL Reputation Service.

        Args:
            phishtank_api_key: Optional API key for PhishTank
            custom_whitelist: Optional set of custom whitelisted domains
            enable_domain_age_check: Whether to check domain age
            enable_ssl_inspection: Whether to inspect SSL certificates
        """
        # Initialize threat feeds
        self._feeds: List[ThreatFeed] = [
            OpenPhishFeed(),
            PhishTankFeed(api_key=phishtank_api_key),
            URLhausFeed(),
        ]

        # Initialize components
        self._whitelist = WhitelistManager(custom_whitelist)
        self._domain_age_checker = (
            DomainAgeChecker() if enable_domain_age_check else None
        )
        self._ssl_inspector = SSLInspector() if enable_ssl_inspection else None

        # Combined bloom filter for all feeds
        self._combined_bloom = BloomFilter(size=2_000_000)

        # Cache for recent lookups
        self._result_cache: Dict[str, Tuple[ReputationResult, datetime]] = {}
        self._cache_ttl = timedelta(minutes=5)

        # Update tracking
        self._last_update: Optional[datetime] = None
        self._update_lock = asyncio.Lock()
        self._is_updating = False

        # Statistics
        self._stats = {
            "openphish_count": 0,
            "phishtank_count": 0,
            "urlhaus_count": 0,
            "total_checks": 0,
            "cache_hits": 0,
            "threats_detected": 0,
        }

        logger.info("URLReputationService initialized")

    async def check_url(self, url: str) -> ReputationResult:
        """
        Check URL against threat databases.

        Args:
            url: URL to check

        Returns:
            ReputationResult with threat assessment
        """
        self._stats["total_checks"] += 1

        # Extract domain
        domain = self._extract_domain(url)
        if not domain:
            return ReputationResult(
                url=url, domain="", is_malicious=False, confidence=0.0
            )

        # Check cache first
        cache_key = url.lower()
        if cache_key in self._result_cache:
            result, cached_at = self._result_cache[cache_key]
            if datetime.now(timezone.utc) - cached_at < self._cache_ttl:
                self._stats["cache_hits"] += 1
                result.cached = True
                return result

        # Check whitelist first
        if self._whitelist.is_whitelisted(domain):
            result = ReputationResult(
                url=url,
                domain=domain,
                is_malicious=False,
                is_whitelisted=True,
                confidence=1.0,
            )
            self._result_cache[cache_key] = (result, datetime.now(timezone.utc))
            return result

        # Initialize result
        is_malicious = False
        threat_type = None
        sources: List[str] = []
        confidence = 0.0

        # Fast bloom filter check
        if self._combined_bloom.contains(url) or self._combined_bloom.contains(domain):
            # Detailed check against each feed
            for feed in self._feeds:
                detected_threat = await feed.check_url(url)
                if detected_threat:
                    is_malicious = True
                    threat_type = detected_threat
                    sources.append(feed.name)
                    confidence = max(confidence, 0.9)

        # Check domain age if enabled
        domain_age_days = None
        if self._domain_age_checker:
            domain_age_days = await self._domain_age_checker.get_domain_age(domain)
            if self._domain_age_checker.is_new_domain(domain_age_days):
                sources.append("domain_age")
                # New domain increases suspicion but isn't definitive
                confidence = max(confidence, 0.3)

        # Check SSL certificate if enabled
        ssl_info = None
        if self._ssl_inspector:
            ssl_info = await self._ssl_inspector.inspect_certificate(domain)
            if not ssl_info.valid:
                sources.append("ssl_inspection")
                confidence = max(confidence, 0.2)
            if ssl_info.valid and ssl_info.organization_match:
                # Good SSL reduces suspicion
                confidence = max(0, confidence - 0.1)

        # Determine final malicious status
        if is_malicious:
            self._stats["threats_detected"] += 1

        result = ReputationResult(
            url=url,
            domain=domain,
            is_malicious=is_malicious,
            threat_type=threat_type,
            sources=sources,
            domain_age_days=domain_age_days,
            ssl_info=ssl_info,
            cached=False,
            confidence=confidence,
        )

        # Cache result
        self._result_cache[cache_key] = (result, datetime.now(timezone.utc))

        return result

    async def check_urls(self, urls: List[str]) -> Dict[str, ReputationResult]:
        """
        Batch check multiple URLs.

        Args:
            urls: List of URLs to check

        Returns:
            Dict mapping URL to ReputationResult
        """
        results = {}

        # Process in parallel with concurrency limit
        semaphore = asyncio.Semaphore(10)

        async def check_with_limit(url: str):
            async with semaphore:
                return url, await self.check_url(url)

        tasks = [check_with_limit(url) for url in urls]
        completed = await asyncio.gather(*tasks, return_exceptions=True)

        for item in completed:
            if isinstance(item, Exception):
                logger.error(f"Batch check error: {item}")
                continue
            url, result = item
            results[url] = result

        return results

    async def update_cache(self) -> UpdateResult:
        """
        Update local threat cache from all feeds.

        Requirement 12.2: Update threat data incrementally every 15 minutes

        Returns:
            UpdateResult with update statistics
        """
        if self._is_updating:
            return UpdateResult(
                success=False,
                entries_added=0,
                entries_removed=0,
                duration_seconds=0,
                errors=["Update already in progress"],
            )

        async with self._update_lock:
            self._is_updating = True
            start_time = datetime.now(timezone.utc)
            errors: List[str] = []
            total_added = 0

            try:
                # Clear combined bloom filter for fresh data
                old_count = self._combined_bloom.item_count
                self._combined_bloom.clear()

                # Fetch from all feeds
                for feed in self._feeds:
                    try:
                        urls = await feed.fetch_threats()

                        # Add to combined bloom filter
                        for url in urls:
                            self._combined_bloom.add(url)
                            domain = self._extract_domain(url)
                            if domain:
                                self._combined_bloom.add(domain)

                        # Update stats
                        if feed.source == ThreatSource.OPENPHISH:
                            self._stats["openphish_count"] = len(urls)
                        elif feed.source == ThreatSource.PHISHTANK:
                            self._stats["phishtank_count"] = len(urls)
                        elif feed.source == ThreatSource.URLHAUS:
                            self._stats["urlhaus_count"] = len(urls)

                        total_added += len(urls)

                    except Exception as e:
                        error_msg = f"{feed.name} update failed: {e}"
                        errors.append(error_msg)
                        logger.error(error_msg)

                self._last_update = datetime.now(timezone.utc)
                duration = (self._last_update - start_time).total_seconds()

                logger.info(
                    f"Cache update complete: {total_added} entries in {duration:.2f}s"
                )

                return UpdateResult(
                    success=len(errors) == 0,
                    entries_added=total_added,
                    entries_removed=old_count,
                    duration_seconds=duration,
                    errors=errors,
                )

            finally:
                self._is_updating = False

    async def start_auto_update(self) -> None:
        """
        Start automatic cache updates every 15 minutes.

        Requirement 12.2: Update threat data incrementally every 15 minutes
        """
        logger.info("Starting automatic cache updates")

        # Initial update
        await self.update_cache()

        # Schedule periodic updates
        while True:
            await asyncio.sleep(self.UPDATE_INTERVAL)
            try:
                await self.update_cache()
            except Exception as e:
                logger.error(f"Auto-update failed: {e}")

    def get_cache_stats(self) -> CacheStats:
        """
        Get statistics about the threat cache.

        Returns:
            CacheStats with current cache information
        """
        return CacheStats(
            total_entries=self._combined_bloom.item_count,
            bloom_filter_size=self._combined_bloom.size,
            last_update=self._last_update,
            openphish_count=self._stats["openphish_count"],
            phishtank_count=self._stats["phishtank_count"],
            urlhaus_count=self._stats["urlhaus_count"],
            false_positive_rate=self._combined_bloom.estimated_false_positive_rate,
        )

    def get_whitelist_manager(self) -> WhitelistManager:
        """Get the whitelist manager for direct access"""
        return self._whitelist

    def _extract_domain(self, url: str) -> Optional[str]:
        """Extract domain from URL"""
        try:
            parsed = urlparse(url)
            return parsed.netloc.lower()
        except Exception:
            return None

    # ============= Whitelist Management Methods =============

    def add_to_whitelist(self, domain: str) -> None:
        """Add a domain to the whitelist"""
        self._whitelist.add_to_whitelist(domain)

    def remove_from_whitelist(self, domain: str) -> bool:
        """Remove a domain from the whitelist"""
        return self._whitelist.remove_from_whitelist(domain)

    def is_whitelisted(self, domain: str) -> bool:
        """Check if a domain is whitelisted"""
        return self._whitelist.is_whitelisted(domain)


# ============= Factory Function =============


def create_url_reputation_service(
    phishtank_api_key: Optional[str] = None,
    custom_whitelist: Optional[Set[str]] = None,
    enable_domain_age_check: bool = True,
    enable_ssl_inspection: bool = True,
) -> URLReputationService:
    """
    Factory function to create a configured URLReputationService.

    Args:
        phishtank_api_key: Optional API key for PhishTank
        custom_whitelist: Optional set of custom whitelisted domains
        enable_domain_age_check: Whether to check domain age
        enable_ssl_inspection: Whether to inspect SSL certificates

    Returns:
        Configured URLReputationService instance
    """
    return URLReputationService(
        phishtank_api_key=phishtank_api_key,
        custom_whitelist=custom_whitelist,
        enable_domain_age_check=enable_domain_age_check,
        enable_ssl_inspection=enable_ssl_inspection,
    )
